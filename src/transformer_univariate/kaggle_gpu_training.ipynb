{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.modules.paths import get_project_root\n",
    "from src.transformer_univariate.config import config\n",
    "from src.transformer_univariate.model import Transformer\n",
    "\n",
    "X = torch.load(get_project_root() /\"src\" / \"transformer_univariate\" / \"data\"  / \"X_train_all.pt\")\n",
    "\n",
    "y = torch.load(get_project_root() /\"src\" / \"transformer_univariate\" / \"data\" / \"y_train_all.pt\")\n",
    "X_test = torch.load(get_project_root() /\"src\" / \"transformer_univariate\" / \"data\"  / \"X_test_all.pt\")[:1000]\n",
    "y_test = torch.load(get_project_root() /\"src\" / \"transformer_univariate\" / \"data\" / \"y_test_all.pt\")[:1000]\n",
    "\n",
    "\n",
    "print(config.dict())\n",
    "\n",
    "m = Transformer(config)\n",
    "if config.load_model:\n",
    "    m.load_state_dict(torch.load(Path(config.path_model) / \"weights.pt\"))\n",
    "    print(\"Model loaded from\", Path(config.path_model) / \"weights.pt\")\n",
    "\n",
    "#BS = 10\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=config.learning_rate)\n",
    "#loss_fn = nn.MSELoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_training_l = []\n",
    "loss_val_l = []\n",
    "\n",
    "for i in range(config.epochs):\n",
    "    idx=torch.randint(0, X.size()[0], (config.batch_size,))\n",
    "    #m.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    out = m(X[idx].squeeze())\n",
    "    # some broadcasting magic (from mingpt)\n",
    "    loss = loss_fn(out.view(config.batch_size*config.block_size,config.vocab_size),y[idx].view(config.batch_size*config.block_size))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % config.evaluation_steps==0:\n",
    "        loss_training = loss.item()\n",
    "        loss_training_l.append(loss_training)\n",
    "        with torch.no_grad():\n",
    "            m=m.eval()\n",
    "            out = m(X_test.squeeze())\n",
    "            loss = loss_fn(out.view(-1,config.vocab_size),y_test.view(-1))\n",
    "            loss_val = loss.item()\n",
    "            loss_val_l.append(loss_val)\n",
    "            m.train()\n",
    "            print(\"i:\", i, \"Loss train\", loss_training, \"Loss val\", loss_val)\n",
    "            torch.save(m.state_dict(), Path(config.path_model) / \"weights.pt\")\n",
    "\n",
    "# save model\n",
    "torch.save(m.state_dict(), Path(config.path_model) / \"weights.pt\")\n",
    "# save config as json\n",
    "import json\n",
    "with open(Path(config.path_model)  / \"config.json\", \"w\") as f:\n",
    "    json.dump(config.json(), f)\n",
    "\n",
    "# save losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_training_l, label=\"training\")\n",
    "plt.plot(loss_val_l, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.savefig(Path(config.path_model) / \"loss.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
